{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysam\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from enformer_pytorch import Enformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import h5py\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts')\n",
    "\n",
    "import config\n",
    "import sequenceUtils\n",
    "\n",
    "import importlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(sequenceUtils)\n",
    "\n",
    "#Set arguments from config file.\n",
    "arguments = {}\n",
    "arguments[\"refGenomePath\"] = config.filePaths.get(\"refGenomePath\")\n",
    "arguments[\"coordStoreDirectory\"] = config.filePaths.get(\"coordStoreDirectory\")\n",
    "arguments[\"enformerOutputStoreFile\"] = config.filePaths.get(\"enformerOutputStoreFile\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The device used is : {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "startIndexList = []\n",
    "fileNamesList = []\n",
    "\n",
    "#Store the range of indices for each file in a dictionary. Only open the concerned file when get_item is called for an index.\n",
    "def createFileNameIndexList():\n",
    "    coordFilesDirectory = os.fsencode(arguments[\"coordStoreDirectory\"])\n",
    "    currentIndex = 0\n",
    "    for file in os.listdir(coordFilesDirectory):\n",
    "        filename = os.fsencode(file).decode(\"utf-8\")\n",
    "        startIndexList.append(currentIndex)\n",
    "        fileNamesList.append(filename)\n",
    "\n",
    "        numItemsFile = getNumberLinesInFile(filename)\n",
    "        currentIndex = currentIndex + numItemsFile\n",
    "\n",
    "    print(f\"The current index is {currentIndex}\")\n",
    "    return startIndexList, fileNamesList\n",
    "\n",
    "def getNumberLinesInFile(filename):\n",
    "    filePath = arguments[\"coordStoreDirectory\"] + \"/\" + filename\n",
    "    with h5py.File(filePath, 'r') as f:\n",
    "        trainingSamples = f[\"trainingCoords\"][:]\n",
    "        numSamples = len(trainingSamples)\n",
    "        return numSamples\n",
    "\n",
    "\"\"\"\n",
    "Samples from all files are clubbed together to get the final samples set. A given index used by dataloader could belong to any file. startIndexList is a list of the start index of all the 492 files.\n",
    "Iterate over this startIndexList and find out which position the indexToFind becomes greater than the startIndex.\n",
    "The file for which index started at this startIndex will be the one containing the indexToFind. So just get the position of the\n",
    "startIndex which is just below the indexToFind and check in the fileNamesList for this position.\n",
    "\"\"\"\n",
    "def getFilePositionFromIndex(startIndexList, indexToFind):\n",
    "   for i, index in enumerate(startIndexList):\n",
    "      if(indexToFind < index):\n",
    "         #This means we moved to the next file, so we have to pick the i before that\n",
    "         filePosition = i-1\n",
    "         return filePosition\n",
    "\n",
    "   #this scenario will bever occur unless indexToFind is a negative value.\n",
    "   return -1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getOneHotEncodedSequenceFromCoordinates(coord):\n",
    "    referenceGenome = pysam.FastaFile(arguments[\"refGenomePath\"])\n",
    "    coords = (coord[0].decode('UTF-8'), int(coord[1]), int(coord[2]))\n",
    "    #Get surrounding sequence for feeding into enformer.\n",
    "    extendedCoordsForEnformer = sequenceUtils.getCoordsForEnformer(coords, referenceGenome)\n",
    "\n",
    "    #Get the raw sequence using the coordinates and the reference genome.\n",
    "    cfDnaFragment = sequenceUtils.getSequenceFromCoord(referenceGenome, extendedCoordsForEnformer)\n",
    "\n",
    "    #One hot encode sequence\n",
    "    encodedFragment = sequenceUtils.oneHotEncodeSequence(cfDnaFragment)\n",
    "\n",
    "    encoded_input_sequence = torch.tensor(np.float32(encodedFragment))\n",
    "    # encoded_input_sequence = encoded_input_sequence.to(device)\n",
    "    return encoded_input_sequence\n",
    "\n",
    "#Look into how much h5py content can be compressed. Greater the compression, longer the time needed to read it again.\n",
    "def storeAsH5pyFile(enformerOutputToStore, labelsToStore):\n",
    "   print(\"Inside store as h5py file\")\n",
    "   with h5py.File(arguments[\"enformerOutputStoreFile\"], 'w') as h5_file:\n",
    "      h5_file.create_dataset(\"enformerOutput\", data=enformerOutputToStore, compression=\"gzip\", compression_opts=8)\n",
    "      h5_file.create_dataset(\"labels\", data=labelsToStore, compression=\"gzip\", compression_opts=8)\n",
    "      h5_file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set the model to eval mode first and then send it to cuda. This prevents the GPU node from running out of memory.\n",
    "enformerModel = Enformer.from_pretrained('EleutherAI/enformer-official-rough', use_checkpointing = True).eval()\n",
    "enformerModel = enformerModel.to(device)\n",
    "nbins = 2\n",
    "ntracks = 5313\n",
    "\n",
    "def getEnformerPredictions(enformer_model, sequence):\n",
    "    print(\"Inside get enformer prediction !!\")\n",
    "    with torch.no_grad():\n",
    "        pretrained_output = enformer_model(sequence)['human'][:, 448:450, :]\n",
    "\n",
    "    # print(f\"Printing the output shape from enformer {pretrained_output.shape}\", flush=True)\n",
    "\n",
    "    #Combine enformer outputs from 2 bins into a single long output. Each bin, each track is a feature. So the total\n",
    "    #number of features for training is num_bins * num_tracks_per_bin. All features can be combined in a\n",
    "    #single 1D tensor array. The other dimension will be the batch size.\n",
    "    print(\"Finished enformer prediction\")\n",
    "    batch_size, nbins, ntracks = pretrained_output.shape\n",
    "\n",
    "    pretrained_output = torch.reshape(pretrained_output, (batch_size, nbins * ntracks))\n",
    "    return pretrained_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EnformerInputDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"Initializing the enformer input dataset\")\n",
    "        inputBedFilesDirectory = os.fsencode(arguments[\"inputBedFileFolder\"])\n",
    "        self.inputDirectory = inputBedFilesDirectory\n",
    "        self.startIndexList, self.fileNamesList = createFileNameIndexList\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filePosition = getFileNameFromIndex(self.startIndexList, index)\n",
    "        filename = self.fileNamesList[filePosition]\n",
    "        indexWithinFile = index - startIndexList[filePosition]\n",
    "\n",
    "        filepath = os.path.join(arguments[\"coordStoreDirectory\"].decode(\"utf-8\"), filename)\n",
    "\n",
    "        with h5py.File(filePath, 'r') as f:\n",
    "            coord = f['trainingCoords'][indexWithinFile]\n",
    "\n",
    "            #Each sample should have only one label, it should be a single value instead of a numpy 1D array.The [0] is to make it a single value instead of a numpy array.\n",
    "            # label = f['trainingLabels'][index][0]\n",
    "            label = f['trainingLabels'][:][indexWithinFile]\n",
    "\n",
    "            encoded_input_sequence = getOneHotEncodedSequenceFromCoordinates(coord)\n",
    "\n",
    "            #For some cases, the coordinates look fine, but the sequence fetched from the fasta file has size 0.\n",
    "            #If we pass such samples to enformer for predictions, we get Einops error, due to dimension mismatch.\n",
    "            try:\n",
    "                assert encoded_input_sequence.shape == (196607,4)\n",
    "\n",
    "            except:\n",
    "                print(f\"One of the samples did not have the right dimensions. The sample index is {index}, shape is {encoded_input_sequence.shape}\")\n",
    "                return torch.empty((196607,4)), label\n",
    "\n",
    "            return encoded_input_sequence, label\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(arguments[\"dataFile\"], 'r') as f:\n",
    "            label = f['trainingLabels'][:]\n",
    "            return label.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The function returns 2 numpy arrays. The 1st numpy array is the enformer output for all cfdna fragments. The second numpy array is the array of labels for all cfDNA fragments.\n",
    "def getEnformerOutput():\n",
    "    problem_indices = []\n",
    "\n",
    "    #For each bin from enformer output that is considered, the output array from enformer will have ntracks number of values. So the total number of values in enformer output array for a single cfDNA fragment will be nbins * ntracks.\n",
    "    enformer_output_size = (1, nbins*ntracks)\n",
    "    enformer_output_array = np.empty(enformer_output_size)\n",
    "\n",
    "    enformerInputDataset = EnformerInputDataset()\n",
    "    enformerInputDataloader = DataLoader(enformerInputDataset, batch_size=16, num_workers=2)\n",
    "\n",
    "    for i, data in enumerate(enformerInputDataloader, 0):\n",
    "        print(f\"Processing data for batch {i}\", flush = True)\n",
    "\n",
    "        encodedSequence, label = data\n",
    "\n",
    "        #Encoded sequence will have the dimension of [batch_size, 196607, 4]. Get item would have been called batch_size\n",
    "        #number of times. Encoded_input_sequence for some batch items might be empty tensors. So iterate over the encodedSequence\n",
    "        #and remove all the rows which have empty encoded_input_sequence?\n",
    "        # print(f\"Printing the shape of the encoded sequence {encodedSequence.shape}\", flush = )\n",
    "        # print(f\"Printing the shape of label {label.shape}\")\n",
    "\n",
    "        encodedSequence = encodedSequence.to(device)\n",
    "\n",
    "        #The 1st dimension is the batch number. In this case, we have only a single batch, so take the 1st element from the batch dimension directly.\n",
    "        enformerPrediction = getEnformerPredictions(enformerModel, encodedSequence)\n",
    "\n",
    "        #the enformer prediction is still in the GPU (since we sent the enformer model and one hot encoded sequence to the GPU. Numpy arrays are not supported in the GPU(GPU probably supports only tensors). So we pass the enformer prediction to CPU and convert it into a numpy array.\n",
    "        #Detach is used to remove the gradients from the predictions. Gradients are similar to the weights of the model. In our case, we are only interested in the predictions and not the model training, so we remove the gradients to save space.\n",
    "        enformerPrediction = enformerPrediction.detach().cpu().numpy()\n",
    "        print(f\"Finished processsing batch {i}, enformer prediction shape is {enformerPrediction.shape}\")\n",
    "        storeAsH5pyFile(enformerPrediction, label)\n",
    "\n",
    "        # enformer_output_array = np.concatenate((enformer_output_array, enformerPrediction))\n",
    "\n",
    "        # all_inputs = torch.cat(all_inputs, dim = 0)\n",
    "\n",
    "\n",
    "    # #Because we are row wise concatenating the enformer predictions to an empty array, the 1st row is the empty array\n",
    "    # #and it consists of only dummy values. Remove the 1st row from the enformer_output_array\n",
    "    # enformer_output_array = enformer_output_array[1:, :]\n",
    "    # print(f\"Problematic counts are {problem_counts} and size is {len(problem_counts)}\")\n",
    "\n",
    "    return -1, -1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    enformerOutputArray, labels = getEnformerOutput()\n",
    "    print(f\"Size of enformer output and labels is {enformerOutputArray.shape}, {labels.shape}\")\n",
    "    # storeAsH5pyFile(enformerOutputArray, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
