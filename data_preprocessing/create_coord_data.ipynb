{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### This file contains functions to read bed files containing cell-free DNA fragment coordinates and store them as H5PY files. During the process, the coordinates are split into training, validation and test sets based on their chromosome number\n",
    "**NOTE**:\n",
    "- Each bed file corresponds to the donor or recipient cfDNA coordinates for a single patient. Donor cfDNA originate from the transplanted lung and recipient cfDNA coordinates originate from the recipient's blood cells.\n",
    "- H5PY output files are created for each coordinate bed file. The coordinates belonging to train, validation and test sets for a single bed file are stored as three separate datasets within the corresponding H5PY output file.\n",
    "- In addition to splitting the dataset into three sets, fragments that are too close to the ends of the chromosome are also removed (because we run into errors while extending coordinates to reach Enformer input size) refer to the comments in the relevant function for more information\n",
    "\n",
    "**Reason behind splitting based on chromosomes**\n",
    "The reason is to avoid data leakage between the training, test and validation sets. cfDNA fragment sequences could arise from the same region and hence have a lot of overlap (depending on the sequencing depth). Thus, even if there is no sample overlap between the three sets, there could be sequence overlap arising from two different fragments coming from the same genomic region. One way of mitigating this is to ensure that no chromosomes are shared between the three sets.\n",
    "\n",
    "*Details on how chromosome-based splitting is done*\n",
    "We need to split the 23 chromosomes between the training, validation and test sets such that the percentage of total samples that belongs to all the chromosomes in a particular set match the sample percentage.\n",
    "1. For chromosome 1 - 23, count the number of samples with that chromosome and calculate what percentage of the total samples this is.\n",
    "2. Start from the chromosome 1 and iterate over all the chromosomes and their percentage samples until training percent is reached. Repeat the process for validation set and assign the rest of the chromosomes to the test set. This should result in 3 lists of chromosomes - for training, validation and test set.\n",
    "3. For each bed file, assign the samples to one of these three sets based on which of the three chromosome lists it is from."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import h5py\n",
    "import pysam\n",
    "\n",
    "#If we don't do this step, then local files like config will not be detected. \n",
    "sys.path.insert(0,'/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts')\n",
    "\n",
    "import config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "\n",
    "#Set arguments from config file. \n",
    "arguments = {}\n",
    "arguments[\"donorFile\"] = config.filePaths.get(\"donorFile\")\n",
    "arguments[\"inputBedFileFolder\"] = config.filePaths.get(\"inputBedFileFolder\")\n",
    "arguments['coordStoreDirectory'] = config.filePaths.get(\"coordStoreDirectory\")\n",
    "arguments['snpFilePath'] = config.filePaths.get(\"snpFile\")\n",
    "arguments['coordsStoreFilePath'] = config.filePaths.get(\"coordStoreFile\")\n",
    "arguments[\"refGenomePath\"] = config.filePaths.get(\"refGenomePath\")\n",
    "\n",
    "arguments['testPercent'] = config.dataCreationConfig.get(\"percentTest\")\n",
    "arguments['validationPercent'] = config.dataCreationConfig.get(\"percentValidation\")\n",
    "arguments['numColsToExtract'] = config.dataCreationConfig.get(\"numColsToExtract\")\n",
    "arguments[\"balanceClassesBeforeCreatingCoordiantes\"] = config.dataCreationConfig.get(\"balanceClassesBeforeCreatingCoordiantes\")\n",
    "\n",
    "#Datasetnames\n",
    "arguments[\"trainingCoordsDatasetName\"] = config.datasetNames.get(\"trainingCoords\")\n",
    "arguments[\"validationCoordsDatasetName\"] = config.datasetNames.get(\"validationCoords\")\n",
    "arguments[\"testCoordsDatasetName\"] = config.datasetNames.get(\"testCoords\")\n",
    "arguments[\"trainingLabelsDatasetName\"] = config.datasetNames.get(\"trainingLabels\")\n",
    "arguments[\"validationLabelsDatasetName\"] = config.datasetNames.get(\"validationLabels\")\n",
    "arguments[\"testLabelsDatasetName\"] = config.datasetNames.get(\"testLabels\")\n",
    "\n",
    "#For ease of testing. If you only want to test, set this flag to False. Then H5PY coordinate files will not be created. \n",
    "WRITE_TO_FILES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input -\n",
    "    1. dataNumpy is a numpy array with chrom number, start and end coordinates as columns for cfDNA fragment samples (rows)\n",
    "    2. label - 0/1. The label is 1 if the samples are from a donor file and 0 if they are from a recipient file\n",
    "\n",
    "Output - A 1D numpy array of 0's or 1's corresponding to the label, each value represanting the label for one sample. Consequently the length of the output array is the same as the number of rows in dataNumpy.\n",
    "'''\n",
    "def getLabelsForData(dataNumpy, label):\n",
    "    nrows, ncols = dataNumpy.shape\n",
    "    if label == 0:\n",
    "        return np.zeros(nrows).reshape(nrows, 1)\n",
    "    if label == 1:\n",
    "        return np.ones(nrows).reshape(nrows, 1)\n",
    "    else:\n",
    "        print(f\"Invalid label for data : {label}\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "'''\n",
    "The config file only contains testPercent (what percent of the non-training data should be test) and validationPercent(what percentage of the total data should be validation).\n",
    "This function calculates the training percentage from these values and returns the absolute training, validation and test percentage (interms of total number of samples).\n",
    "\n",
    "Output - 3 percentage values (training, validation and test)\n",
    "'''\n",
    "def getSampleDistributionPercents():\n",
    "    testPercent = arguments[\"testPercent\"]\n",
    "    validationPercent = arguments[\"validationPercent\"]\n",
    "    \n",
    "    nonTestPercent = (100 - testPercent)\n",
    "    absValidationPercent = nonTestPercent * validationPercent/100\n",
    "    absTrainingPercent = nonTestPercent * (100 - validationPercent)/100\n",
    "    print(f\"Abolsute training validation and test percentages are {absTrainingPercent}, {absValidationPercent} and {testPercent}\")\n",
    "    return (absTrainingPercent, absValidationPercent, testPercent)\n",
    "\n",
    "'''\n",
    "Inputs -\n",
    "1. testPercent - percentage of total samples that should belong to the test set.\n",
    "2. inputBedFilesDirectory - path to the directory containing input bed files.\n",
    "Output - File names that should belong to the test set.\n",
    "Description: Function iterates over files, counting number of samples until the required test percentage is attained. All the patient file names that were counted to reach the test percentage are returned.\n",
    "'''\n",
    "def getTestPatientsList(testPercent, inputBedFilesDirectory):\n",
    "    total_samples = 0\n",
    "    test_patient_filenames = []\n",
    "\n",
    "    #Get the total number of samples in the bed files directory\n",
    "    for filename in os.listdir(inputBedFilesDirectory):\n",
    "        filepath = os.path.join(inputBedFilesDirectory, filename)\n",
    "        columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "        cfdna_frag_df = pd.read_csv(filepath, sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "        total_samples += len(cfdna_frag_df)\n",
    "    \n",
    "    req_test_samples = math.floor(testPercent/100 * total_samples)\n",
    "    print(f\"Number of test samples required : {req_test_samples} and total samples is {total_samples}\")    \n",
    "    \n",
    "    #Get the filename until which the sample count reaches the test percentage level\n",
    "    num_samples = 0\n",
    "    for filename in os.listdir(inputBedFilesDirectory):\n",
    "        if \"recipient\" in filename: continue\n",
    "\n",
    "        donor_file_path = os.path.join(inputBedFilesDirectory, filename)\n",
    "        recipient_file_path = donor_file_path.replace(\"donor\", \"recipient\")\n",
    "        recipient_file_name = filename.replace(\"donor\", \"recipient\")\n",
    "\n",
    "        columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "        donor_cf_dna_df = pd.read_csv(donor_file_path, sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "        recipient_cf_dna_df = pd.read_csv(recipient_file_path, sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "\n",
    "        num_samples += len(donor_cf_dna_df) + len(recipient_cf_dna_df)\n",
    "        test_patient_filenames.append(filename)\n",
    "        test_patient_filenames.append(recipient_file_name)\n",
    "\n",
    "        if(num_samples > req_test_samples):\n",
    "            print(f\"Reached the required test samples, at filename {filename} and the current num_samples is {num_samples}\")\n",
    "            break\n",
    "    \n",
    "    return test_patient_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The functions in the following block are all involved in splitting the samples into training, validation and test sets based on their chromosome.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input -\n",
    "1. inputBedFilesDirectoryPath - directory path to where all the bed files are\n",
    "2. columnNames - column names in the bed file (to later convert to dataframe)\n",
    "3. trainPercent - percentage of total samples in training set\n",
    "\n",
    "Output - 2 lists of chromosomes - for the training and validation sets.\n",
    "'''\n",
    "def getChromosomeListsForTrainingAndValidation(inputBedFilesDirectoryPath, columnNames, trainPercent, testPatients):\n",
    "    average_percentage_df = getChromosomePercentagesAverage(inputBedFilesDirectoryPath, testPatients, columnNames)\n",
    "    print(f\"Total of percentage of the df is {average_percentage_df['percentage of samples'].sum()}\")\n",
    "    (training_end_index, training_chromosomes) = getChromosomesCoveringPercentSamples(average_percentage_df, trainPercent)\n",
    "    print(f\"Training end index is {training_end_index}\")\n",
    "    validation_chromosomes = average_percentage_df.loc[training_end_index + 1:][\"#chrom\"].values.tolist()\n",
    "    return (training_chromosomes, validation_chromosomes)\n",
    "\n",
    "'''\n",
    "Input\n",
    "1. TestPatients - list of filenames that belong to the test set (from previous function)\n",
    "2. inputBedFilesDirectoryPath - directory path to where all the bed files are\n",
    "3. columnNames - column names in the bed file (to later convert to dataframe)\n",
    "\n",
    "Output - a dataframe with 2 columns - #chrom and percentage of samples.\n",
    "\n",
    "Iterates over patient files and calculates the percentage (of total samples in that file) that belong to a chromosome.\n",
    "For each chromosome, the percentages are added for all the files and later divided by the total number of files to get the percentage of total samples that belong to a chromosome (averaged over all patients)\n",
    "'''\n",
    "def getChromosomePercentagesAverage(inputBedFilesDirectoryPath, testPatients, columnNames):\n",
    "    all_samples_df = pd.DataFrame(columns=['#chrom', \"percentage of samples\"])\n",
    "\n",
    "    #Insert chromosome numbers.\n",
    "    chroms = range(1, 23)\n",
    "    list_chroms = list(map(lambda chrom: str(chrom), chroms)) + [\"X\"] + [\"Y\"]\n",
    "    all_samples_df[\"#chrom\"] = list_chroms\n",
    "    all_samples_df[\"percentage of samples\"] = [0] * 24\n",
    "    \n",
    "    num_files = 0\n",
    "    for filename in os.listdir(inputBedFilesDirectoryPath):\n",
    "        if(filename not in testPatients):\n",
    "            filepath = os.path.join(inputBedFilesDirectoryPath, filename)\n",
    "\n",
    "            num_files += 1\n",
    "            cfdna_frag_df = pd.read_csv(filepath, sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "            \n",
    "            #If this string conversion is not done, for some files, #chrom till 14 are not strings. This creates problems while \n",
    "            #matching to the string chromosomes from the all_samples_df\n",
    "            cfdna_frag_df[\"#chrom\"]= cfdna_frag_df[\"#chrom\"].map(str)\n",
    "            cfdna_chrom_sample_count = cfdna_frag_df.groupby(\"#chrom\").size().reset_index()\n",
    "            cfdna_chrom_sample_count.columns = [\"#chrom\", \"percentage of samples\"]\n",
    "\n",
    "            #Transform from count to percentage\n",
    "            total_samples = len(cfdna_frag_df)\n",
    "            cfdna_chrom_sample_count[\"percentage of samples\"] = cfdna_chrom_sample_count[\"percentage of samples\"].transform(lambda x: x/total_samples * 100)\n",
    "            # print(f\"Printing percentage of samples in file {filename} ...  \\n {cfdna_chrom_sample_count.head(25)}\", flush=True)\n",
    "\n",
    "            # Pick the value from the cfdna_chrom_sample_count where #chrom in the chrom_sample_count df matches the #chrom\n",
    "            # of the row being updated in the all_samples_df. The cdfna_chrom_sample_count.loc returns a pandas series. \n",
    "            # values[0][1] is used to fetch the single int/float value of the percentage. \n",
    "            # All samples should contain the sum of percentages from all files for each chromosome.\n",
    "            all_samples_df[\"percentage of samples\"] = all_samples_df.apply(lambda x:  addPercentagesFunction(x[\"#chrom\"], x[\"percentage of samples\"], cfdna_chrom_sample_count), axis = 1)\n",
    "\n",
    "\n",
    "    #Take the average of the percentages sum over all files. \n",
    "    all_samples_df[\"percentage of samples\"] =  all_samples_df[\"percentage of samples\"].transform(lambda x: x/num_files)\n",
    "    \n",
    "    #Check to see if all the percentages in the final all_samples_df add upto 100. \n",
    "    all_samples_avg = all_samples_df[\"percentage of samples\"]\n",
    "    all_samples_avg_sum = all_samples_avg.sum()                         \n",
    "    if(round(all_samples_avg_sum) != 100):\n",
    "        raise Exception(f\"********* Something is wrong !! The sum of percentages of all files combined(${all_samples_avg_sum}) is not adding up to 100. \\n After averaging, the all samples df is {all_samples_df.head(25)}\") \n",
    "    \n",
    "    print(f\"Chromosomes percentage df is {all_samples_df}\")\n",
    "    return all_samples_df\n",
    "\n",
    "'''\n",
    "Inputs\n",
    "1. chrom - chromosome number\n",
    "2. Percentage - percentage of samples arising from #chrom for a patient\n",
    "3. file_level_chrom_percent_df - df of chromosomes and their sample percentages.\n",
    "\n",
    "Add a single patient's sample percentage to the chromosomes to sample percentages df.\n",
    "'''\n",
    "def addPercentagesFunction(chrom, percentage, file_level_chrom_percent_df):\n",
    "    percent_to_add = file_level_chrom_percent_df.loc[file_level_chrom_percent_df[\"#chrom\"] == chrom][\"percentage of samples\"]\n",
    "    if(percent_to_add.values.size == 0):\n",
    "        return percentage\n",
    "    else:\n",
    "        return percentage + percent_to_add.values[0]\n",
    "\n",
    "'''\n",
    "Input -\n",
    "1. chrom_avg_percent_coverage_df - df of chromosome and average percentage of samples that originate from this chromosome\n",
    "2. maxPercent - trainingPercent or validationPercent in this case\n",
    "\n",
    "Given a maxPercentage of samples, it returns a list of chromosomes whose samples when taken together reach that maxPercent\n",
    "\n",
    "Output -\n",
    "1. endIndex - the last chromosome in the list (so for the next set you can count from the next chrom onwards) (need not be returned, refactor method to not return it in later)\n",
    "2. chrom_list - the list of chromosomes whose samples when taken together reach the max percent.\n",
    "'''\n",
    "def getChromosomesCoveringPercentSamples(chrom_avg_percent_coverage_df, maxPercent):\n",
    "    chromosomes_list = []\n",
    "    percent_covered = 0\n",
    "    end_index = -1\n",
    "    for i, row in chrom_avg_percent_coverage_df.iterrows():\n",
    "        chrom = row[\"#chrom\"]\n",
    "        avg_percentage = row[\"percentage of samples\"]\n",
    "        percent_covered = percent_covered + avg_percentage\n",
    "        chromosomes_list.append(chrom)\n",
    "        if(percent_covered > maxPercent):\n",
    "            end_index = i\n",
    "            break\n",
    "    if(end_index) == -1:\n",
    "        raise Exception(\"Something is wrong, the inidividual chromosome percentages is not sufficient to reach the percentage requested\")\n",
    "    \n",
    "    return (end_index, chromosomes_list)\n",
    "\n",
    "'''\n",
    "cfdna_frag_df - df of start, end coordinates and chrom number of cfDNA samples\n",
    "train_chroms - list of chromosomes whose fragments should make the training set\n",
    "validation_chroms - list of chromosomes whose fragments should make the validation set\n",
    "\n",
    "The function splits the cfdna_frag_df into training and validation dfs according to their chromosomes.\n",
    "'''\n",
    "def getTrainingAndValidationData(cfdna_frag_df, train_chroms, validation_chroms):\n",
    "    numColumnsToExtract = arguments[\"numColsToExtract\"]\n",
    "    training_df = cfdna_frag_df.loc[cfdna_frag_df[\"#chrom\"].isin(train_chroms)].iloc[:, 0:numColumnsToExtract]\n",
    "    validation_df = cfdna_frag_df.loc[cfdna_frag_df[\"#chrom\"].isin(validation_chroms)].iloc[:, 0:numColumnsToExtract] \n",
    "    return (training_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enformer expects input size of 196607 bps. cfDNA fragments are much smaller, so we align it on reference genome and take sequences on either side, such that the cfDNA fragment is in the middle\n",
    "If the input sequence is too close to the start or end of the the chromosome, we can't extend it. Shifting to the right, accounting for the changed bins etc is too complex\n",
    "So we simple discard fragments which are too close to the left or the right of the chromosome\n",
    "\"\"\"\n",
    "def removeCloseToEndFrags(df):\n",
    "    #Get the length of each chromosome\n",
    "    refGenomePath = arguments[\"refGenomePath\"]\n",
    "    refGenome = pysam.FastaFile(refGenomePath)\n",
    "\n",
    "    chrom_len_map = {}\n",
    "    chroms = df['#chrom'].unique()\n",
    "    for i in chroms:\n",
    "        chrom = str(i)\n",
    "        length = refGenome.get_reference_length(\"chr\" + chrom)\n",
    "        chrom_len_map[chrom] = length\n",
    "\n",
    "    allowed_distance_from_edges = 100000 #length to be extended on either of fragment for enformer input is 196607/2 = 98303.\n",
    "\n",
    "    #Drop fragments too close to beginning of the chromosome\n",
    "    df = df.drop(df[df['start'] <= allowed_distance_from_edges].index)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #Drop fragments too close to the end of the chromosome\n",
    "    for i in range(len(df)):\n",
    "        chrom = str(df[\"#chrom\"][i])\n",
    "        if(df[\"end\"][i] >= chrom_len_map[chrom] - allowed_distance_from_edges):\n",
    "            df.drop(i)\n",
    "\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Call the function that splits chromosomes for training, validation and test sets. Store the chromosome lists for each of the three sets in variables for later use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "inputBedFilesDirectory = arguments[\"inputBedFileFolder\"]\n",
    "trainingPercent, validationPercent, testPercent = getSampleDistributionPercents()\n",
    "testPatients = getTestPatientsList(testPercent, inputBedFilesDirectory)\n",
    "print(f\"Test patients list is {testPatients}\")\n",
    "\n",
    "#Preparatory step - decide which chromosomes should be part of training, validation and test set\n",
    "#while maintaining the train, validation and test ratio somewhat, and making sure chromosomes are not shared between these 3 sets.\n",
    "#Create the chromosome number vs percentage covered df and use it to get the list of chromosomes for training, validation and test data.\n",
    "#Uncomment it, once testing is done (This has been moved to a previous cell for ease of splitting and running)\n",
    "training_chromosomes, validation_chromosomes = getChromosomeListsForTrainingAndValidation(inputBedFilesDirectory, columnNames, trainingPercent, testPatients)\n",
    "print(f\"Training, validation and test chromosomes are {training_chromosomes}, {validation_chromosomes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The function that ties all the previous small functions together. It calls functions/has code to do the following\n",
    "\n",
    "1. Iterate over donor and recipient files to get dfs out of bed files. Make the length of donor and recipient df equal by truncating the bigger file\n",
    "2. Call function to split the dfs into training and validation df based on their chromosome.\n",
    "3. Call functions to remove fragments that are too close to the end of the chromosome. Also get the test data from the test patient files\n",
    "4. Write all the data into H5PY files within separate training validation and test dataset.\n",
    "'''\n",
    "def fetchCoordinatesAndStore():\n",
    "    rows_to_skip = 11\n",
    "    \n",
    "    columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "    inputBedFilesDirectory = arguments[\"inputBedFileFolder\"]\n",
    "    count_test = 0\n",
    "    count_train_valid = 0\n",
    "    count_files = 0\n",
    "\n",
    "    #Iterate over each file, get the training, validation and test dataset, store to H5PY file.\n",
    "    for filename in os.listdir(inputBedFilesDirectory):\n",
    "        count_files = count_files + 1\n",
    "        print(f\"Processing filename {filename}\")\n",
    "        filepath = os.path.join(inputBedFilesDirectory, filename)\n",
    "\n",
    "        #For test patients, don't split based on chromosomes. Test patients data is directly stored into H5PY coordinate files. \n",
    "        if filename in testPatients:\n",
    "            testCoordStoreFilePath = os.path.join(arguments[\"coordStoreDirectory\"], filename.replace('.frag.bed.gz', '') + \".hdf5\")\n",
    "            test_label = 1 if \"donor\" in filename else 0\n",
    "            count_test += 1\n",
    "            cfdna_frag_df = pd.read_csv(filepath,\n",
    "                        sep = \"\\t\", names = columnNames, skiprows=rows_to_skip)\n",
    "            numColumnsToExtract = arguments[\"numColsToExtract\"]\n",
    "            test_data = cfdna_frag_df.iloc[:, 0:numColumnsToExtract]\n",
    "            test_data = removeCloseToEndFrags(test_data)\n",
    "            testLabels = getLabelsForData(test_data, test_label)\n",
    "            if(WRITE_TO_FILES == True):\n",
    "                with h5py.File(testCoordStoreFilePath, 'w') as h5_file:\n",
    "                    print(f\"Storing into testFile : {testCoordStoreFilePath}, test_data : {len(test_data)} and testLabels: {len(testLabels)}\")\n",
    "                    h5_file.create_dataset(arguments[\"testCoordsDatasetName\"], data=test_data.astype(str).to_numpy(), compression = \"gzip\", compression_opts=9)\n",
    "                    h5_file.create_dataset(arguments[\"testLabelsDatasetName\"], data=testLabels, compression = \"gzip\", compression_opts=9)\n",
    "            continue\n",
    "        \n",
    "        if 'recipient' in filename: continue #Just read donor files\n",
    "        #For testing, when you have to check only the test files\n",
    "        if 'donor' in filename: continue \n",
    "\n",
    "        cfdna_frag_dfs = []\n",
    "        file_paths = []\n",
    "\n",
    "        ##Getting number of samples in donor. Append the cfDNA fragment df into a list (Its a 2 element list with only the donor and the recipient for that patient)\n",
    "        file_paths.append(filepath)\n",
    "        cfdna_frag_dfs.append(pd.read_csv(file_paths[0],\n",
    "                        sep = \"\\t\", names = columnNames, skiprows=rows_to_skip))\n",
    "        donor_sample_length = len(cfdna_frag_dfs[0])\n",
    "\n",
    "        #Get the number of samples in the recipient\n",
    "        file_paths.append(file_paths[0].replace('donor', 'recipient'))\n",
    "        cfdna_frag_dfs.append(pd.read_csv(file_paths[1],\n",
    "                        sep = \"\\t\", names = columnNames, skiprows=rows_to_skip))\n",
    "        recipient_sample_length = len(cfdna_frag_dfs[1])\n",
    "\n",
    "        #get the minimum of the samples between donor and recipient\n",
    "        min_sample_length = min(donor_sample_length, recipient_sample_length)\n",
    "        \n",
    "        #Do the same steps for the donor and the recipient file \n",
    "        for i in range(0,2):\n",
    "            filename = os.path.basename(file_paths[i])\n",
    "\n",
    "            if arguments[\"balanceClassesBeforeCreatingCoordiantes\"]:\n",
    "                cfdna_frag_dfs[i] = cfdna_frag_dfs[i].sample(n=min_sample_length, random_state=42, replace=False)\n",
    "\n",
    "            #Some chromosome values are not proper strings. Convert all #chrom values to strings.\n",
    "            cfdna_frag_dfs[i][\"#chrom\"]= cfdna_frag_dfs[i][\"#chrom\"].map(str)\n",
    "            train_data, validation_data = getTrainingAndValidationData(cfdna_frag_dfs[i], training_chromosomes, validation_chromosomes)\n",
    "\n",
    "            #Remove fragments that are too close to the start and end of the chromosome\n",
    "            train_data = removeCloseToEndFrags(train_data)\n",
    "            validation_data = removeCloseToEndFrags(validation_data)\n",
    "\n",
    "            #Get labels for the data\n",
    "            label = 1 if \"donor\" in filename else 0\n",
    "            trainingLabels = getLabelsForData(train_data, label)\n",
    "            validationLabels = getLabelsForData(validation_data, label)\n",
    "\n",
    "            # print(f”Printing train data characteristics. Length: {len(train_data)}, Head : {train_data.head()}“)\n",
    "            # print(f”Printing validation data characteristics. Length: {len(validation_data)}, Head : {validation_data.head()}“)\n",
    "            # print(f”Size and head of train labels {len(trainingLabels)}, {trainingLabels[0:10]}“)\n",
    "            # print(f”Size and head of validation labels {len(validationLabels)}, {validationLabels[0:10]}“)\n",
    "\n",
    "            #Store the data into H5PY files as separate datasets.\n",
    "            if(WRITE_TO_FILES == True):\n",
    "                coordStoreFilePath = os.path.join(arguments[\"coordStoreDirectory\"], filename.replace('.frag.bed.gz', '') + \".hdf5\")\n",
    "                with h5py.File(coordStoreFilePath, 'w') as h5_file:\n",
    "                    h5_file.create_dataset(arguments[\"trainingCoordsDatasetName\"], data=train_data.astype(str).to_numpy(), compression = \"gzip\", compression_opts=9)\n",
    "                    h5_file.create_dataset(arguments[\"trainingLabelsDatasetName\"], data=trainingLabels, compression = \"gzip\", compression_opts=9)\n",
    "                    h5_file.create_dataset(arguments[\"validationCoordsDatasetName\"], data=validation_data.astype(str).to_numpy(), compression = \"gzip\", compression_opts=9)\n",
    "                    h5_file.create_dataset(arguments[\"validationLabelsDatasetName\"], data=validationLabels, compression = \"gzip\", compression_opts=9)\n",
    "                \n",
    "            count_train_valid += 1\n",
    "\n",
    "    print(f\"Count test: {count_test} and count train valid : {count_train_valid}\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function reads the newly created H5PY files and verifes if everything is in order. The following checks are performed.\n",
    "    1. The number of donor are recipient samples are the same between the bed files and the coordinate files.\n",
    "    2. The number of data samples and the labels is the same for training, validation.\n",
    "    3. The donors have label 1 and the recipients have label 0\n",
    "    4. Number of training, validation and test add up to the minimal sample number used from bed files for each coord file.\n",
    "    5. Number of samples in donor and recipient files (all 3 sets put together are equal) are the same in coordinate file\n",
    "\n",
    "NOTE: This function is not yet modified to reflect the updated split policy of - set aside some patient files as test patients. Only the samples from the remaining patients are split into training and validation sets.\n",
    "\"\"\"\n",
    "def verifyNewCoordinateFiles():\n",
    "\n",
    "    inputBedFilesDirectory = arguments[\"inputBedFileFolder\"]\n",
    "    coordStoreDirectory = arguments[\"coordStoreDirectory\"]\n",
    "    \n",
    "    bed_files = os.listdir(inputBedFilesDirectory)\n",
    "    coord_files = os.listdir(coordStoreDirectory)\n",
    "\n",
    "    #Assertion 1: Assert that the total number of bed files and coordinate files are the same\n",
    "    assert len(bed_files) == len(coord_files), (f\"There are {len(bed_files)} bed files in \" + \n",
    "        f\"{os.path.basename(inputBedFilesDirectory)} and {len(coord_files)} \" + \n",
    "            f\"H5PY files in {os.path.basename(coordStoreDirectory)}. The numbers should match, something is wrong !!\")\n",
    "\n",
    "    #Assertion 2: that the total number of donor and recipient bed and coorinate files are the same\n",
    "    num_donors_bed = 0\n",
    "    num_recips_bed = 0\n",
    "    num_donors_coord = 0\n",
    "    num_recips_coord = 0\n",
    "\n",
    "    for i in range(0, len(bed_files)):\n",
    "        if \"donor\" in bed_files[i]:\n",
    "            num_donors_bed += 1\n",
    "        if \"donor\" in coord_files[i]:\n",
    "            num_donors_coord += 1\n",
    "        if \"recipient\" in bed_files[i]:\n",
    "            num_recips_bed += 1\n",
    "        if \"recipient\" in coord_files[i]:\n",
    "            num_recips_coord += 1\n",
    "\n",
    "    assert num_donors_bed == num_donors_coord, f\"Number of donor bed files ({num_donors_bed}) does not match the number of coordinate bed files({num_donors_coord})\"\n",
    "    assert num_recips_bed == num_recips_coord, f\"Number of donor bed files ({num_recips_bed}) does not match the number of coordinate bed files({num_recips_coord})\"\n",
    "\n",
    "    for i in range(0, len(coord_files)):\n",
    "        columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "        bed_file = bed_files[i]\n",
    "\n",
    "        #The kind of assertions would differ because we are not balancing the test class. Assert for the test patients separately.\n",
    "        if filename in testPatients:\n",
    "            test_data_length = len(f[\"testCoords\"][:])\n",
    "            test_labels = f[\"testLabels\"][:]\n",
    "\n",
    "            assert test_data_length == len(test_labels), (f\"For file {filename},\" +\n",
    "                f\"the length of testdata ({test_data_length}) and length of testlabels ({len(test_labels)}) does not match\")\n",
    "\n",
    "            #Assertion 3 for test data : assert that donors have label 1 and recipients have label 0\n",
    "            test_zeros = np.zeros(test_data_length)\n",
    "            test_ones = np.ones(test_data_length)\n",
    "            test_compare_array = test_ones if \"donor\" in filename else test_zeros\n",
    "            assert np.all(test_labels == test_compare_array), f\"For file {filename}, some test samples have incorrect labels\"\n",
    "\n",
    "            #Assertion 4 equivalient for Test data\n",
    "            #Assert that the number of donor and recipient coords in the coordinate H5PY file is the same as the \n",
    "            #number of donor and recipient coordinates in the bed file \n",
    "\n",
    "        #Process only the donor files\n",
    "        if \"recipient\" in bed_file: continue \n",
    "\n",
    "        print(f\"Processing file {bed_file}\")\n",
    "        donor_bed_file = bed_file\n",
    "        recip_bed_file = donor_bed_file.replace(\"donor\", \"recipient\")\n",
    "        donor_coord_file = donor_bed_file.replace('.frag.bed.gz', '') + \".hdf5\"\n",
    "        recip_coord_file = recip_bed_file.replace('.frag.bed.gz', '') + \".hdf5\"\n",
    "\n",
    "        donor_bed_path = os.path.join(inputBedFilesDirectory, donor_bed_file)\n",
    "        recip_bed_path = os.path.join(inputBedFilesDirectory, recip_bed_file)\n",
    "\n",
    "        donor_cfdna_df = pd.read_csv(donor_bed_path,\n",
    "                        sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "        recip_cfdna_df = pd.read_csv(recip_bed_path,\n",
    "                        sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "        \n",
    "        og_donor_length = len(donor_cfdna_df)\n",
    "        og_recip_length = len(recip_cfdna_df)\n",
    "\n",
    "        coord_lengths_donor_recip = []\n",
    "\n",
    "        for filename in [donor_coord_file, recip_coord_file]:\n",
    "            coord_path = os.path.join(coordStoreDirectory, filename)\n",
    "\n",
    "            with h5py.File(coord_path, 'r') as f:\n",
    "                train_data_length = len(f[\"trainingCoords\"][:])\n",
    "                validation_data_length = len(f[\"validationCoords\"][:])\n",
    "                train_labels = f[\"trainingLabels\"][:]\n",
    "                validation_labels = f[\"validationLabels\"][:]\n",
    "\n",
    "                #Assertion 2: Assert that the number of data samples and labels are the same for training, validation and test set. \n",
    "                assert train_data_length == len(train_labels), (f\"For file {filename},\" + \n",
    "                    f\"the length of trainingdata ({train_data_length}) and length of traininglabels ({len(train_labels)}) does not match\")\n",
    "                assert validation_data_length == len(validation_labels), (f\"For file {filename},\" +\n",
    "                    f\"the length of validation data ({validation_data_length}) and length of validationlabels ({len(validation_labels)}) does not match\")\n",
    "\n",
    "                #Assertion 3: assert that donors have label 1 and recipients have label 0\n",
    "                train_zeros = np.zeros(train_data_length)\n",
    "                validation_zeros = np.zeros(validation_data_length)\n",
    " \n",
    "                train_ones = np.ones(train_data_length)\n",
    "                validation_ones = np.ones(validation_data_length)\n",
    "\n",
    "                if \"donor\" in filename: \n",
    "                    assert np.all(train_labels == train_ones), f\"For file {filename}, some training donor samples have incorrect labels\"\n",
    "                    np.all(validation_labels == validation_ones), f\"For file {filename}, some validation donor samples have incorrect labels\"\n",
    "                \n",
    "                if \"recipient\" in filename:\n",
    "                    np.all(train_labels == train_zeros), f\"For file {filename}, some training recipient samples have incorrect labels\"\n",
    "                    np.all(validation_labels == validation_zeros), f\"For file {filename}, some validation recipient samples have incorrect labels\"\n",
    "\n",
    "                total_coord_length = train_data_length + validation_data_length + test_data_length\n",
    "                coord_lengths_donor_recip.append(total_coord_length)\n",
    "        \n",
    "        #Assertion 4 & 5: Assert the sum of training, validation and test set in the donor coordinate H5PY file is\n",
    "        #the same as the minimum of the donor/recipient file samples in the corresponding bed files. \n",
    "        #Do the same for the recipient coordinate H5PY file. \n",
    "        og_min_length = min(og_donor_length, og_recip_length)\n",
    "        \n",
    "        assert coord_lengths_donor_recip[0] == og_min_length, (f\"minimum of donors and recipient lengths {og_min_length} \" +\n",
    "            f\"does not match the total donor coordinates in the file {donor_coord_file}({coord_lengths_donor_recip[0]})\")\n",
    "\n",
    "        assert coord_lengths_donor_recip[1] == og_min_length, (f\"minimum of donors and recipient lengths {og_min_length}\" +\n",
    "            f\"does not match the total coordinates in the file {recip_coord_file}({coord_lengths_donor_recip[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"Arguments is {arguments}\")\n",
    "    fetchCoordinatesAndStore()\n",
    "    print(f\"================================================================================================================\")\n",
    "    print(f\"Finished storing coordinate files, starting verifications... \")\n",
    "    print(f\"================================================================================================================\")\n",
    "    # verifyNewCoordinateFiles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NOTE: All functions from this point onwards are for testing if certain parts of the code work well and are not part of the main functionality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING:: Get lengths of training and validation data for one file using the methods for splitting based on chromosome**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the training and validation chromosome lists \n",
    "columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "\n",
    "#Skip this if it's already done\n",
    "inputBedFilesDirectory = arguments[\"inputBedFileFolder\"]\n",
    "trainingPercent, validationPercent, testPercent = getSampleDistributionPercents()\n",
    "training_chromosomes, validation_chromosomes = getChromosomeListsForTrainingAndValidation(inputBedFilesDirectory, columnNames, trainingPercent, testPatients)\n",
    "print(f\"Training, validation and test chromosomes are {training_chromosomes}, {validation_chromosomes}\")\n",
    "\n",
    "columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "inputBedFilesDirectory = arguments[\"inputBedFileFolder\"]\n",
    "filename = \"L29-M29-5.donor.frag.bed.gz\"\n",
    "filepath = os.path.join(inputBedFilesDirectory, filename)\n",
    "cfdna_frag_df = pd.read_csv(filepath, sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "train_chroms = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
    "validation_chroms = ['12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y']\n",
    "train_data, valid_data = getTrainingAndValidationData(cfdna_frag_df, train_chroms, validation_chroms)\n",
    "print(len(train_data))\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying if the number of positives, negatives, samples in the coord file is correct wrt the bed file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "coordStoreDir = arguments[\"coordStoreDirectory\"]\n",
    "inputBedFilesDirectory = arguments[\"inputBedFileFolder\"]\n",
    "trainingPercent, validationPercent, testPercent = getSampleDistributionPercents()\n",
    "testPatients = getTestPatientsList(testPercent, inputBedFilesDirectory)\n",
    "\n",
    "for filename in os.listdir(coordStoreDir):\n",
    "    filepath = os.path.join(coordStoreDir, filename)\n",
    "    inputBedFilesPath = os.path.join(inputBedFilesDirectory, filename.replace('.hdf5', '') + \".frag.bed.gz\")\n",
    "    columnNames  = [\"#chrom\", \"start\", \"end\", \"read_id\", \"mapq\", \"cigar1\", \"cigar2\"]\n",
    "    if filename.replace(\".hdf5\", \".frag.bed.gz\") not in testPatients:\n",
    "        testSamples = 0\n",
    "        df = pd.read_csv(inputBedFilesPath, sep = \"\\t\", names = columnNames, skiprows=11)\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # testSamples += len(f[\"testCoords\"][:])\n",
    "            # print(f\"For filename : {filename}, testSamples: {testSamples}\")\n",
    "            # trainingSamples = len(f[\"trainingCoords\"][:])\n",
    "            # validationSamples = len(f[\"validationCoords\"][:])\n",
    "            trainingLabels = f[\"trainingLabels\"][:]\n",
    "            validationLabels = f[\"validationLabels\"][:]\n",
    "            pos_train = (trainingLabels == 1).sum()\n",
    "            neg_train = (trainingLabels == 0).sum()\n",
    "            pos_valid = (validationLabels == 1).sum()\n",
    "            neg_valid = (validationLabels == 0).sum()\n",
    "            print(f\"For filename: {filename}, Pos Train: {pos_train}, Neg Train: {neg_train}, Pos valid: {pos_valid} and neg Valid: {neg_valid}\")\n",
    "            #print(f\"For filename {filename}, Total samples coord : {trainingSamples + validationSamples}. Total samples bed files : {len(df)}\")\n",
    "            # print(f\"Training samples : {trainingSamples} and validationSamples: {validationSamples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying test coords for a patient file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hpc/compgen/projects/fragclass/analysis/mvivekanandan/output/properSplitCoordinateFiles/L21-M23.donor.hdf5\n",
      "3055\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join(coordStoreDir, \"L21-M23.donor.hdf5\")\n",
    "print(filepath)\n",
    "with h5py.File(filepath, 'r') as f:\n",
    "    print(len(f[\"testCoords\"][:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING:: Test contents of the Coordinate H5PY files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Testing the contents of h5py file.\n",
    "with h5py.File(\"/hpc/compgen/projects/fragclass/analysis/mvivekanandan/output/coordinateFiles/L4-M36.donor.hdf5\", 'r') as f:\n",
    "    trainingData = f['trainingCoords'][:]\n",
    "    validationData = f[\"validationCoords\"][:]\n",
    "    testData = f[\"testCoords\"][:]\n",
    "    print(len(trainingData), len(validationData), len(testData))\n",
    "    print(trainingData[0:10, :])\n",
    "    print(validationData[0:10, :])\n",
    "    print(testData[0:10, :])\n",
    "    \n",
    "    print(\"Switching to labels\")\n",
    "    trainingLabel = f[\"trainingLabels\"][:]\n",
    "    validationLabel = f[\"validationLabels\"][:]\n",
    "    testLabel = f[\"testLabels\"][:]\n",
    "    print(len(trainingLabel), len(validationLabel), len(testLabel))\n",
    "    print(trainingLabel[0:5], validationLabel[0:5], testLabel[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move files from test patients to a separate directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "coordStoreDir = arguments[\"coordStoreDirectory\"]\n",
    "testPatientsDir = \"/hpc/compgen/projects/fragclass/analysis/mvivekanandan/output/testCoordFilesEndFragRemoved\"\n",
    "testPatients\n",
    "for filename in os.listdir(coordStoreDir):\n",
    "    filepath = os.path.join(coordStoreDir, filename)\n",
    "    if(filename.replace(\"hdf5\", \"frag.bed.gz\") in testPatients):\n",
    "        # shutil.copy(filepath, testPatientsDir)\n",
    "        os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fragenv] *",
   "language": "python",
   "name": "conda-env-fragenv-py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
