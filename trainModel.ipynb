{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysam\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from enformer_pytorch import Enformer\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts')\n",
    "\n",
    "import config\n",
    "import sequenceUtils\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(sequenceUtils)\n",
    "\n",
    "#Set arguments from config file.\n",
    "arguments = {}\n",
    "arguments[\"enformerOutputStoreFile\"] = config.filePaths.get(\"enformerOutputStoreFile\")\n",
    "arguments[\"batchSize\"] = config.modelHyperParameters.get(\"batchSize\")\n",
    "arguments[\"learningRate\"] = config.modelHyperParameters.get(\"learningRate\")\n",
    "arguments[\"numberOfWorkers\"] = config.modelHyperParameters.get(\"numberOfWorkers\")\n",
    "arguments[\"numberEpochs\"] = config.modelHyperParameters.get(\"numberEpochs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The device used is : {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BasicDenseLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicDenseLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * 5313, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 200)\n",
    "        self.fc3 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.enformerOutputFilePath = arguments[\"enformerOutputStoreFile\"]\n",
    "\n",
    "    \"\"\"\n",
    "    The indexes fetched by dataloader iteration are not in order, because shuffling is set to true. This will not cause a mismatch\n",
    "    between the enformer output and the label. Because enformer output and label are fetched for the same index, so they will still\n",
    "    correspond to each other.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.enformerOutputFilePath, 'r') as f:\n",
    "\n",
    "            #Will the whole data be loaded everytime we have to fetch an index ? Should be not ! Thats what h5py files are for right ?\n",
    "            enformerOutput = f['enformerOutput']\n",
    "            # print(f\"Inside get item, index is {index} and the total num samples is {enformerOutput.shape}\\n\")\n",
    "\n",
    "            enformerOutput = enformerOutput[index]\n",
    "            # print(f\"Shape of enformer output of index {index} is {enformerOutput.shape}\")\n",
    "            encoded_enformer_output = torch.tensor(np.float32(enformerOutput))\n",
    "\n",
    "            #Each sample should have only one label, it should be a single value instead of a numpy 1D array.\n",
    "            labels = f['labels']\n",
    "            # print(f\"Just retrieved the labels from the file, the shape of labels is {labels.shape}\")\n",
    "            label = labels[index][0]\n",
    "\n",
    "        return encoded_enformer_output, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return 140\n",
    "        # with h5py.File(self.enformerOutputFilePath, 'r') as f:\n",
    "        #     h5py_dataset = f[\"labels\"]\n",
    "        #     return len(h5py_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def objectiveFn(batchSize, learningRate, numWorkers, numEpochs):\n",
    "\n",
    "    #Set the model to eval mode first and then send it to cuda. This prevents the GPU node from running out of memory.\n",
    "    enformerModel = Enformer.from_pretrained('EleutherAI/enformer-official-rough', use_checkpointing = True).eval()\n",
    "    enformerModel = enformerModel.to(device)\n",
    "\n",
    "    trainingDataset = TrainingDataset()\n",
    "    trainingDataloader = DataLoader(trainingDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "    denseLayerModel = BasicDenseLayer().to(device)\n",
    "\n",
    "    #Define the loss function and optimizer.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(denseLayerModel.parameters(), lr=learningRate, momentum=0.9)\n",
    "\n",
    "    loss_list = []\n",
    "    #Training the model\n",
    "    for epoch in range(numEpochs):\n",
    "        trainingFn(epoch, criterion, optimizer, denseLayerModel, trainingDataloader, enformerModel, loss_list)\n",
    "\n",
    "    print(f\"Printing the type of loss list{type(loss_list)}\")\n",
    "\n",
    "    print(f\"Finished training, type of loss_list is {loss_list}\")\n",
    "    xs = [x for x in range(len(loss_list))]\n",
    "    plt.plot(xs, loss_list)\n",
    "    plt.show()\n",
    "    print(\"Training is complete !!! Starting validations \")\n",
    "    #Validation of model predictions\n",
    "    # validationDataset = ValidationDataset()\n",
    "    # validationFn(validationDataset, criterion, denseLayerModel, enformerModel)\n",
    "\n",
    "\n",
    "def trainingFn(epoch, criterion, optimizer, denseLayerModel, dataloader, enformerModel, loss_list):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        count = count + 1\n",
    "        print(f\"Inside epoch {epoch}, Dataloader index is {i}\")\n",
    "        enformerPrediction, label = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            #While creating torch.tensor, device can be passed as cuda. But that was a suspect for GPU node running out of memory.\n",
    "            #After iterating through dataset and fetching each sample, send the labels and sequence to cuda,\n",
    "            enformerPrediction = enformerPrediction.to(device)\n",
    "\n",
    "        print(f\"After batching, printing enformer prediction and label shapes: {enformerPrediction.shape}, {label.shape}\\n\", flush=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        modelPrediction = denseLayerModel(enformerPrediction).to(device)\n",
    "        print(f\"The shape of model prediction is {modelPrediction.shape}\\n\",flush=True)\n",
    "        print(f\"The model prediction is {modelPrediction}\\n\", flush=True)\n",
    "\n",
    "        #Without this conversion, model throws RuntimeError: Expected Scalar type Long but found Float Error.\n",
    "        label = label.type(torch.LongTensor).to(device)\n",
    "\n",
    "        print(f\"The shape of the label is {label.size()}\\n\", flush=True)\n",
    "        print(f\"The label is {label}\\n\")\n",
    "\n",
    "        # Get cross entropy loss between model's prediction and true label.\n",
    "        loss = criterion(modelPrediction, label)\n",
    "\n",
    "        print(f\"Just computed the loss for epoch {epoch} and dataloader iteration {i}. It is {loss}\\n\", flush=True)\n",
    "\n",
    "        # Backward pass and calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Uses the gradients from backward pass to nudge the learning weights.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss for every training set\n",
    "        # Check that the loss is continuosly decreasing over training samples.\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # if i % 500 == 0:\n",
    "        print(f\"Inside Training function, the running loss is {running_loss}\\n\",flush=True)\n",
    "\n",
    "    #The final running loss should be divided by this number of to get the average running loss.\n",
    "    print(f\"Total number of iterations for the current epoch is {count}\\n\")\n",
    "\n",
    "    #The running_loss is the sum of individual losses for each batch.\n",
    "    #The average running loss for the epoch should be runnning_loss divided by the number of batches.\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"Finished iterating through the dataLoader for epoch {epoch}. The number of batches is {num_batches} and running loss is {running_loss}\\n\",flush=True)\n",
    "    avg_running_loss = running_loss/num_batches\n",
    "    print(f\"Average running loss for epoch {epoch} after backward pass is {avg_running_loss}\\n\")\n",
    "    loss_list.append(avg_running_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"The arguments is {arguments}\")\n",
    "    batchsize = arguments[\"batchSize\"]\n",
    "    learningRate = arguments[\"learningRate\"]\n",
    "    numWorkers = arguments[\"numberOfWorkers\"]\n",
    "    numEpochs = arguments[\"numberEpochs\"]\n",
    "    print(f\"The arguments are {batchsize}, {learningRate}, {numWorkers}, {numEpochs}\")\n",
    "\n",
    "    objectiveFn(batchsize, learningRate, numWorkers, numEpochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
