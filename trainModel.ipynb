{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This file has functions for the following\n",
    "1. Define a Basic dense layer network\n",
    "2. Create a dataset which reads the enformerOutput from the H5PY file and returns batches of the enformer predictions (as training data)\n",
    "3. Run the basic dense layer model for the enformer predictions training data.\n",
    "4. Run the trained model for the validation dataset\n",
    "5. Plot loss for the training and validation dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysam\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from enformer_pytorch import Enformer\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "sys.path.insert(0,'/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts')\n",
    "\n",
    "import config\n",
    "import sequenceUtils\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(sequenceUtils)\n",
    "\n",
    "#Set arguments from config file.\n",
    "arguments = {}\n",
    "arguments[\"trainingEnformerOutputStoreFile\"] = config.filePaths.get(\"trainingEnformerOutputStoreFile\")\n",
    "arguments[\"validationEnformerOutputStoreFile\"] = config.filePaths.get(\"validationEnformerOutputStoreFile\")\n",
    "arguments[\"testEnformerOutputStoreFile\"] = config.filePaths.get(\"testEnformerOutputStoreFile\")\n",
    "arguments[\"batchSize\"] = config.modelHyperParameters.get(\"batchSize\")\n",
    "arguments[\"learningRate\"] = config.modelHyperParameters.get(\"learningRate\")\n",
    "arguments[\"numberOfWorkers\"] = config.modelHyperParameters.get(\"numberOfWorkers\")\n",
    "arguments[\"numberEpochs\"] = config.modelHyperParameters.get(\"numberEpochs\")\n",
    "arguments[\"modelStateStoreDirectory\"] = config.filePaths.get(\"modelStateStoreDirectory\")\n",
    "arguments[\"lossFunctionPlotDirectory\"] = config.filePaths.get(\"lossFunctionPlotDirectory\")\n",
    "print(arguments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The device used is : {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic Dense Layer network. Consists of 3 linear layers. Size of input is 2*5313 and the size of the output is 2.\n",
    "Relu functions are placed between each of these layers.\n",
    "\"\"\"\n",
    "class BasicDenseLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicDenseLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * 5313, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 200)\n",
    "        self.fc3 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset which reads the H5PY file containing enformer out (enformerOutputStoreFile provided in config file) and returns the output along with the labels\n",
    "\n",
    "Ouptut from get_item method - a tuple of encoded_enformer_output and label. encoded_enformer_output is a 1D torch tensor of size (10626). label is a single integer of 1 or 0 denoting whether the cfDNA fragment came from a tumour tissue or a regular tissue.\n",
    "\"\"\"\n",
    "class EnformerInputDataset(Dataset):\n",
    "        def __init__(self, sampleType):\n",
    "        self.sampleType = sampleType\n",
    "        self.enformerOutputDatasetName = sampleType + \"EnformerOutput\"\n",
    "        self.labelsDatasetName = sampleType + \"Labels\"\n",
    "\n",
    "        self.enformerOutputFileKey = sampleType + \"EnformerOutputStoreFile\"\n",
    "        self.enformerOutputFilePath = arguments[self.enformerOutputFileKey]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.enformerOutputFilePath, 'r') as f:\n",
    "\n",
    "            enformerOutput = f['enformerOutput']\n",
    "\n",
    "            enformerOutput = enformerOutput[index]\n",
    "            encoded_enformer_output = torch.tensor(np.float32(enformerOutput))\n",
    "\n",
    "            labels = f['labels']\n",
    "            label = labels[index][0]\n",
    "\n",
    "        return encoded_enformer_output, label\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.enformerOutputFilePath, 'r') as f:\n",
    "            h5py_dataset = f[\"labels\"]\n",
    "            return len(h5py_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plotLossFunction(loss_list, modelname):\n",
    "    plotPath = os.path.join(arguments[\"lossFunctionPlotDirectory\"], modelname)\n",
    "    xs = [x for x in range(len(loss_list))]\n",
    "    plt.plot(xs, loss_list)\n",
    "    plt.savefig(plotPath)\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Create a file(the name of the file has the current date and time) and save the state_dict of the model\n",
    "\"\"\"\n",
    "def saveModel(model):\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    filename_extension = now.strftime(\"%d_%m_%H_%M_%S\")\n",
    "    print(\"date and time =\", filename_extension)\n",
    "\n",
    "    filename = \"trained_model_\" + filename_extension\n",
    "    filepath = os.path.join(arguments[\"modelStateStoreDirectory\"], filename)\n",
    "\n",
    "    f = open(filepath, \"x\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    f.close()\n",
    "    return filename_extension\n",
    "\n",
    "\"\"\"\n",
    "For each epoch, this function is responsible for training the basic dense layer network with enformer predictions data.\n",
    "The running_loss for each batch is added and the average_running_loss for the epoch in question is calculated by dividing the total running loss with the number of samples.\n",
    "The average running loss for the epoch is added to the loss list.\n",
    "\n",
    "Args:\n",
    "epoch(int) : The current epoch for which training is performed.\n",
    "criterion(function) - The loss function for training.\n",
    "optimizer(function) - Optimizer function for back propagation.\n",
    "denseLayerModel - the dense layer model to be trained.\n",
    "dataloader - the dataloader object. This object iterates over all samples and creates batches of enformer output data for training\n",
    "loss_lost - list of floats. Has the ongoing loss_list for each epoch.\n",
    "\"\"\"\n",
    "def getModelPredictionAndLoss(denseLayerModel, dataloader, criterion, lossList, isTraining=False, epoch=False, optimizer = False):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    time_to_train = 0\n",
    "    start_time = time.time()\n",
    "    print(f\"Start time is {start_time}\")\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        print(f\"Inside epoch {epoch}, Dataloader index is {i}\")\n",
    "        enformerPrediction, label = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            #While creating torch.tensor, device can be passed as cuda. But that was a suspect for GPU node running out of memory.\n",
    "            #After iterating through dataset and fetching each sample, send the labels and sequence to cuda,\n",
    "            enformerPrediction = enformerPrediction.to('cuda')\n",
    "            label = label.type(torch.LongTensor).to('cuda')\n",
    "\n",
    "        # print(f\"After batching, printing enformer prediction and label shapes: {enformerPrediction.shape}, {label.shape}\\n\", flush=True)\n",
    "\n",
    "        t = time.time()\n",
    "        modelPrediction = denseLayerModel(enformerPrediction)\n",
    "\n",
    "        #Without this conversion, model throws RuntimeError: Expected Scalar type Long but found Float Error.\n",
    "\n",
    "        # Get cross entropy loss between model's prediction and true label.\n",
    "        loss = criterion(modelPrediction, label)\n",
    "\n",
    "        #If the model is being trained, then do backpropagation and calculate loss.\n",
    "        if(isTraining):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass and calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Uses the gradients from backward pass to nudge the learning weights.\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        time_to_train += (time.time() - t)\n",
    "\n",
    "        # Print loss for every training set\n",
    "        # Check that the loss is continuosly decreasing over training samples.\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # if i % 500 == 0:\n",
    "        # print(f\"Inside Training function, the running loss epoch {epoch} for the batch{i} is {running_loss}\\n\",flush=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"End time is {end_time}\")\n",
    "    total_time = end_time - start_time\n",
    "    time_to_load = total_time - time_to_train\n",
    "\n",
    "    #The running_loss is the sum of individual losses for each batch.\n",
    "    #The average running loss for the epoch should be runnning_loss divided by the number of batches.\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"Finished iterating through the dataLoader for epoch {epoch}. The number of batches is {num_batches} and running loss is {running_loss}\\n\",flush=True)\n",
    "    avg_running_loss = running_loss/num_batches\n",
    "    print(f\"Average running loss for epoch {epoch} after backward pass is {avg_running_loss}\\n\")\n",
    "    lossList.append(avg_running_loss)\n",
    "    print(f\"Average time to train model per batch for epoch {epoch} is {time_to_train/num_batches}\")\n",
    "    print(f\"Average time to load input per batch for epoch {epoch} is {time_to_load/num_batches}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function does the following\n",
    "1. Creates dataloader for the trainingDataset. The dataset in turn returns the enformer predictions read from enformerOuput h5py file.\n",
    "2. Iterates over the number of epochs and calls the training function for getting the predictions of the Basic dense layer model for the enformer output.\n",
    "3. Gets predictions of the trained model for the validation datastet.\n",
    "4. Plots the loss function for the training and validation dataset.\n",
    "\n",
    "Args:\n",
    "batchSize(int) - the batch size for training. This value is read from the config file.\n",
    "learningRate(float) - learning rate for training. This value is again read from the config file.\n",
    "numWorkers(int) - number of parallel CPU processes that can be run for loading the data, functions etc. Also read from config file.\n",
    "numEpochs(int) - number of epochs for training. Also read from the config file\n",
    "\"\"\"\n",
    "def objectiveFn(batchSize, learningRate, numWorkers, numEpochs):\n",
    "    denseLayerModel = BasicDenseLayer().to('cuda')\n",
    "\n",
    "    trainingDataset = EnformerOutputDataset(\"training\")\n",
    "    trainingDataloader = DataLoader(trainingDataset, batch_size=batchSize, num_workers=numWorkers)\n",
    "\n",
    "    validationDataset = EnformerOutputDataset(\"validation\")\n",
    "    validationDataloader = DataLoader(validationDataset, batch_size=batchSize, num_workers=numWorkers)\n",
    "\n",
    "    #Define the loss function and optimizer.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(denseLayerModel.parameters(), lr=learningRate, momentum=0.9)\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    #Training the model\n",
    "    for epoch in range(numEpochs):\n",
    "        getModelPredictionAndLoss(denseLayerModel, trainingDataloader, criterion, loss_list, True, epoch, optimizer)\n",
    "\n",
    "    #Checking size of model's weights for each layer\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in denseLayerModel.state_dict():\n",
    "        print(f\"For param {param_tensor}, the model's state dict size is {denseLayerModel.state_dict()[param_tensor].size()}\")\n",
    "\n",
    "    modelname = saveModel(denseLayerModel)\n",
    "    plotLossFunction(loss_list, modelname)\n",
    "\n",
    "    #Validating model\n",
    "    print(\"Training is complete ! Starting validations\")\n",
    "    with torch.no_grad():\n",
    "        getModelPredictionAndLoss(denseLayerModel.eval(), validationDataloader, criterion, loss_list)\n",
    "    print(f\"Completed validation, the average per batch loss for validation is {loss_list[-1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batchsize = arguments[\"batchSize\"]\n",
    "    learningRate = arguments[\"learningRate\"]\n",
    "    numWorkers = arguments[\"numberOfWorkers\"]\n",
    "    numEpochs = arguments[\"numberEpochs\"]\n",
    "\n",
    "    objectiveFn(batchsize, learningRate, numWorkers, numEpochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
