{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This file has functions for the following\n",
    "1. Define a Basic dense layer network\n",
    "2. Create a dataset which reads the enformerOutput from the H5PY file and returns batches of the enformer predictions (as training data)\n",
    "3. Run the basic dense layer model for the enformer predictions training data.\n",
    "4. Run the trained model for the validation dataset\n",
    "5. Plot loss for the training and validation dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysam\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from enformer_pytorch import Enformer\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts')\n",
    "\n",
    "import config\n",
    "import sequenceUtils\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(sequenceUtils)\n",
    "\n",
    "#Set arguments from config file.\n",
    "arguments = {}\n",
    "arguments[\"enformerOutputStoreFile\"] = config.filePaths.get(\"enformerOutputStoreFile\")\n",
    "arguments[\"batchSize\"] = config.modelHyperParameters.get(\"batchSize\")\n",
    "arguments[\"learningRate\"] = config.modelHyperParameters.get(\"learningRate\")\n",
    "arguments[\"numberOfWorkers\"] = config.modelHyperParameters.get(\"numberOfWorkers\")\n",
    "arguments[\"numberEpochs\"] = config.modelHyperParameters.get(\"numberEpochs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The device used is : {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic Dense Layer network. Consists of 3 linear layers. Size of input is 2*5313 and the size of the output is 2.\n",
    "Relu functions are placed between each of these layers.\n",
    "\"\"\"\n",
    "class BasicDenseLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicDenseLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * 5313, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 200)\n",
    "        self.fc3 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset which reads the H5PY file containing enformer out (enformerOutputStoreFile provided in config file) and returns the output along with the labels\n",
    "\n",
    "Ouptut from get_item method - a tuple of encoded_enformer_output and label. encoded_enformer_output is a 1D torch tensor of size (10626). label is a single integer of 1 or 0 denoting whether the cfDNA fragment came from a tumour tissue or a regular tissue.\n",
    "\"\"\"\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.enformerOutputFilePath = arguments[\"enformerOutputStoreFile\"]\n",
    "\n",
    "    \"\"\"\n",
    "    The indexes fetched by dataloader iteration are not in order, because shuffling is set to true. This will not cause a mismatch\n",
    "    between the enformer output and the label. Because enformer output and label are fetched for the same index, so they will still\n",
    "    correspond to each other.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.enformerOutputFilePath, 'r') as f:\n",
    "\n",
    "            enformerOutput = f['enformerOutput']\n",
    "\n",
    "            enformerOutput = enformerOutput[index]\n",
    "            encoded_enformer_output = torch.tensor(np.float32(enformerOutput))\n",
    "\n",
    "            labels = f['labels']\n",
    "            label = labels[index][0]\n",
    "\n",
    "        return encoded_enformer_output, label\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.enformerOutputFilePath, 'r') as f:\n",
    "            h5py_dataset = f[\"labels\"]\n",
    "            return len(h5py_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function does the following\n",
    "1. Creates dataloader for the trainingDataset. The dataset in turn returns the enformer predictions read from enformerOuput h5py file.\n",
    "2. Iterates over the number of epochs and calls the training function for getting the predictions of the Basic dense layer model for the enformer output.\n",
    "3. Gets predictions of the trained model for the validation datastet.\n",
    "4. Plots the loss function for the training and validation dataset.\n",
    "\n",
    "Args:\n",
    "batchSize(int) - the batch size for training. This value is read from the config file.\n",
    "learningRate(float) - learning rate for training. This value is again read from the config file.\n",
    "numWorkers(int) - number of parallel CPU processes that can be run for loading the data, functions etc. Also read from config file.\n",
    "numEpochs(int) - number of epochs for training. Also read from the config file\n",
    "\"\"\"\n",
    "def objectiveFn(batchSize, learningRate, numWorkers, numEpochs):\n",
    "    trainingDataset = TrainingDataset()\n",
    "    trainingDataloader = DataLoader(trainingDataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "    denseLayerModel = BasicDenseLayer().to(device)\n",
    "\n",
    "    #Define the loss function and optimizer.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(denseLayerModel.parameters(), lr=learningRate, momentum=0.9)\n",
    "\n",
    "    loss_list = []\n",
    "    #Training the model\n",
    "    for epoch in range(numEpochs):\n",
    "        trainingFn(epoch, criterion, optimizer, denseLayerModel, trainingDataloader, loss_list)\n",
    "\n",
    "    xs = [x for x in range(len(loss_list))]\n",
    "    plt.plot(xs, loss_list)\n",
    "    plt.show()\n",
    "    print(\"Training is complete !!! Starting validations \")\n",
    "\n",
    "    #Validation of model predictions\n",
    "    # validationDataset = ValidationDataset()\n",
    "    # validationFn(validationDataset, criterion, denseLayerModel)\n",
    "\n",
    "\"\"\"\n",
    "For each epoch, this function is responsible for training the basic dense layer network with enformer predictions data.\n",
    "The running_loss for each batch is added and the average_running_loss for the epoch in question is calculated by dividing the total running loss with the number of samples.\n",
    "The average running loss for the epoch is added to the loss list.\n",
    "\n",
    "Args:\n",
    "epoch(int) : The current epoch for which training is performed.\n",
    "criterion(function) - The loss function for training.\n",
    "optimizer(function) - Optimizer function for back propagation.\n",
    "denseLayerModel - the dense layer model to be trained.\n",
    "dataloader - the dataloader object. This object iterates over all samples and creates batches of enformer output data for training\n",
    "loss_lost - list of floats. Has the ongoing loss_list for each epoch.\n",
    "\"\"\"\n",
    "def trainingFn(epoch, criterion, optimizer, denseLayerModel, dataloader, loss_list):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        count = count + 1\n",
    "        print(f\"Inside epoch {epoch}, Dataloader index is {i}\")\n",
    "        enformerPrediction, label = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            #While creating torch.tensor, device can be passed as cuda. But that was a suspect for GPU node running out of memory.\n",
    "            #After iterating through dataset and fetching each sample, send the labels and sequence to cuda,\n",
    "            enformerPrediction = enformerPrediction.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        modelPrediction = denseLayerModel(enformerPrediction).to(device)\n",
    "\n",
    "        #Without this conversion, model throws RuntimeError: Expected Scalar type Long but found Float Error.\n",
    "        label = label.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # Get cross entropy loss between model's prediction and true label.\n",
    "        loss = criterion(modelPrediction, label)\n",
    "\n",
    "        print(f\"Just computed the loss for epoch {epoch} and dataloader iteration {i}. It is {loss}\\n\", flush=True)\n",
    "\n",
    "        # Backward pass and calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Uses the gradients from backward pass to nudge the learning weights.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss for every training set\n",
    "        # Check that the loss is continuosly decreasing over training samples.\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "        print(f\"Running loss for sample index {i} inside epoch {epoch} is {running_loss}\\n\",flush=True)\n",
    "\n",
    "\n",
    "    #The running_loss is the sum of individual losses for each batch.\n",
    "    #The average running loss for the epoch should be runnning_loss divided by the number of batches.\n",
    "    num_batches = len(dataloader)\n",
    "    avg_running_loss = running_loss/num_batches\n",
    "    print(f\"Average running loss for epoch {epoch} after backward pass is {avg_running_loss}\\n\")\n",
    "    loss_list.append(avg_running_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batchsize = arguments[\"batchSize\"]\n",
    "    learningRate = arguments[\"learningRate\"]\n",
    "    numWorkers = arguments[\"numberOfWorkers\"]\n",
    "    numEpochs = arguments[\"numberEpochs\"]\n",
    "\n",
    "    objectiveFn(batchsize, learningRate, numWorkers, numEpochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
