{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is a test script to generate the Enformer tracks for entire length of chromosome 1. Enformer takes an input of size 196607 bps. This script contains functions to break down the entire chromosome into Enformer input lengths and store the results in a H5PY file.\n",
    "\n",
    "Refer to the file generate_enformer_tracks_chr1_reference_genome.py for more details."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from enformer_pytorch import Enformer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import h5py\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts')\n",
    "\n",
    "import config\n",
    "import utils\n",
    "\n",
    "import importlib   \n",
    "import os\n",
    "import time\n",
    "\n",
    "import pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(utils)\n",
    "\n",
    "arguments = {}\n",
    "\n",
    "#File paths\n",
    "arguments[\"refGenomePath\"] = config.filePaths.get(\"refGenomePath\")\n",
    "\n",
    "#Enformer output model hyperparameters\n",
    "arguments[\"enformerBatchSize\"] = config.modelHyperParameters.get(\"enformerBatchSize\")\n",
    "arguments[\"enformerNumberOfWorkers\"] = config.modelHyperParameters.get(\"enformerNumberOfWorkers\")\n",
    "\n",
    "#General configs\n",
    "arguments[\"file_sharing_strategy\"] = config.modelGeneralConfigs.get(\"fileSharingStrategy\")\n",
    "arguments[\"enformerOutputFileCompression\"] = config.modelGeneralConfigs.get(\"enformerOutputFileCompression\")\n",
    "arguments[\"enformerOutputFileChunkSize\"] = config.modelGeneralConfigs.get(\"enformerOutputFileChunkSize\")\n",
    "\n",
    "#Datasets\n",
    "arguments[\"trainingLabelsDatasetName\"] = config.datasetNames.get(\"trainingLabels\")\n",
    "arguments[\"validationLabelsDatasetName\"] = config.datasetNames.get(\"validationLabels\")\n",
    "arguments[\"testLabelsDatasetName\"] = config.datasetNames.get(\"testLabels\")\n",
    "arguments[\"trainingEnformerOutputDatasetName\"] = config.datasetNames.get(\"trainingEnformerOutput\")\n",
    "arguments[\"validationEnformerOutputDatasetName\"] = config.datasetNames.get(\"validationEnformerOutput\")\n",
    "arguments[\"testEnformerOutputDatasetName\"] = config.datasetNames.get(\"testEnformerOutput\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "referenceGenome = pysam.FastaFile(arguments[\"refGenomePath\"])\n",
    "coords = (\"1\", \"10000\", \"10010\")\n",
    "sequence = utils.getSequenceFromCoord(referenceGenome, coords)\n",
    "print(len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enformerSizedBitsRefGenome(Dataset):\n",
    "    def __init__(self):\n",
    "        self.papa = \"papa\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # print(f\"About to fetch the encoded fragment for {index}\")\n",
    "        start = 196607 * (index - 1)\n",
    "        end = 196607 * index\n",
    "        coords = (1, start, end)\n",
    "        sequence = utils.getSequenceFromCoord(referenceGenome, coords)\n",
    "        encodedFragment = utils.oneHotEncodeSequence(sequence)\n",
    "        encodedFragment = torch.tensor(np.float32(encodedFragment))\n",
    "        # print(f\"For index {index}, coords are {coords} and shape of the encoded fragment is {encodedFragment.shape}\")\n",
    "        return encodedFragment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnformerPredictions(enformer_model, sequence, ntracks):\n",
    "    # print(\"Inside get enformer prediction !!\")\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #For each output from enformer, get the right bin. \n",
    "        full_enformer_output = enformer_model(sequence)['human']\n",
    "    \n",
    "    #the enformer prediction is still in the GPU (since we sent the enformer model and one hot encoded sequence to the GPU. Numpy arrays are not supported in the GPU(GPU probably supports only tensors). So we pass the enformer prediction to CPU and convert it into a numpy array.\n",
    "    #Detach is used to remove the gradients from the predictions. Gradients are similar to the weights of the model. In our case, we are only interested in the predictions and not the model training, so we remove the gradients to save space.\n",
    "    full_enformer_output = full_enformer_output.detach().cpu()\n",
    "    batch_size, nbins, ntracks = full_enformer_output.shape\n",
    "    final_enformer_output = torch.empty(ntracks).view(1, -1)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(nbins):\n",
    "            single_bin_track = full_enformer_output[i, j, :].view(1, -1)\n",
    "            final_enformer_output = torch.cat((final_enformer_output, single_bin_track), dim = 0)\n",
    "    \n",
    "    return final_enformer_output[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look into how much h5py content can be compressed. Greater the compression, longer the time needed to read it again.\n",
    "def storeAsH5pyFile(numSamples, numEnformerOuputSingleSample, createDataset = False, h5_file = False, \n",
    "                    enformerOutputToStore=False, currentIndex = False):\n",
    "   \n",
    "   dataset_name = \"refGenomeEnformerOutputs\"\n",
    "   h5py_file_path = \"/hpc/compgen/projects/fragclass/analysis/mvivekanandan/output/refGenomeEnformerOutput.h5py\"\n",
    "   \n",
    "   num_h5py_samples = numSamples * 896\n",
    "   #If we opening the H5PY file for the 1st time then create the dataset and return the file. \n",
    "   if createDataset: \n",
    "      print(\"This is the 1st time. Inside createDataset\")\n",
    "\n",
    "      if h5_file == False:\n",
    "         h5_file = h5py.File(h5py_file_path, \"w-\")\n",
    "\n",
    "      h5_file.create_dataset(dataset_name, (num_h5py_samples, numEnformerOuputSingleSample),\n",
    "                                    compression=\"gzip\", compression_opts=arguments[\"enformerOutputFileCompression\"],\n",
    "                                      chunks = (arguments[\"enformerOutputFileChunkSize\"], numEnformerOuputSingleSample))\n",
    "      return(h5_file)\n",
    "\n",
    "   else:\n",
    "      sizeOfOutputToStore = len(enformerOutputToStore)\n",
    "      endIndex = currentIndex + sizeOfOutputToStore\n",
    "      h5_file[dataset_name][(currentIndex):(endIndex),:] = enformerOutputToStore\n",
    "      return endIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_worker_sharing_strategy(worker_id: int) -> None:\n",
    "    torch.multiprocessing.set_sharing_strategy(arguments[\"file_sharing_strategy\"])\n",
    "\n",
    "\n",
    "#The function returns 2 numpy arrays. The 1st numpy array is the enformer output for all cfdna fragments. The second numpy array is the array of labels for all cfDNA fragments.\n",
    "def storeEnformerOutput(h5_file = False):\n",
    "    torch.multiprocessing.set_sharing_strategy(arguments[\"file_sharing_strategy\"])\n",
    "\n",
    "    nbins = 2\n",
    "    ntracks = 5313\n",
    "\n",
    "    #Set the model to eval mode first and then send it to cuda. This prevents the GPU node from running out of memory.\n",
    "    enformerModel = Enformer.from_pretrained('EleutherAI/enformer-official-rough', use_checkpointing = True).eval()\n",
    "    enformerModel = enformerModel.to(device)\n",
    "    \n",
    "    enformerInputDataset = enformerSizedBitsRefGenome()\n",
    "    enformerInputDataloader = DataLoader(enformerInputDataset, batch_size=arguments[\"enformerBatchSize\"], \n",
    "                                        num_workers=arguments[\"enformerNumberOfWorkers\"],\n",
    "                                        shuffle=True, worker_init_fn=set_worker_sharing_strategy)\n",
    "    \n",
    "    print(f\"number of batches : {len(enformerInputDataloader)}\")\n",
    "    numSamples = len(enformerInputDataset)\n",
    "\n",
    "    # #Create the datasets for storing enformer output. \n",
    "    h5_file = storeAsH5pyFile(numSamples, ntracks, True, h5_file)\n",
    "    currentH5Index = 0\n",
    "\n",
    "    for i, data in enumerate(enformerInputDataloader):\n",
    "        \n",
    "        #Store the filepath and the index within file to a separate CSV file. This is to ensure that we are able to locate the sample\n",
    "        #so we can access the metadata(from original coordinate bed file) associated with the sample. \n",
    "        #filepath and index should have all the samples data from this batch. \n",
    "        encodedSequence = data\n",
    "        \n",
    "        # print(f\"Printing the shape of the encoded sequence {encodedSequence.shape}\", flush = True)\n",
    "        # print(f\"Printing the shape of label {label.shape}\")\n",
    "        encodedSequence = encodedSequence.to(device)\n",
    "        \n",
    "        #Will be of the shape [batch_size * 10626]\n",
    "        enformerPrediction = getEnformerPredictions(enformerModel, encodedSequence, ntracks).detach().cpu().numpy()\n",
    "    \n",
    "        #The data is getting too big to load, round off enformer predictions to 3 decimal places. \n",
    "        enformerPrediction = np.around(enformerPrediction, decimals=3)\n",
    "        print(f\"Size of enformer output to be stored in h5py file is {enformerPrediction.shape}\")\n",
    "        \n",
    "        \"\"\"\n",
    "        H5 file contents are updated every batch. To ensure that the contents are not overwritten every batch, store with indices. \n",
    "        The indices given are ascending order numbers starting from 0, this ensures that the shuffled order is maintained while storing in H5PY file. \n",
    "        \"\"\"\n",
    "        currentH5Index = storeAsH5pyFile(numSamples, ntracks, False, h5_file, enformerPrediction, currentH5Index)\n",
    "        print(f\"Finished processing batch {i}. The number of samples stored in H5PY file so far is {currentH5Index}\", flush = True)\n",
    "\n",
    "    h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 1696003171.766663\n",
      "number of batches : 159\n",
      "This is the 1st time. Inside createDataset\n",
      "Got the encoded sequence\n",
      "Size of enformer output to be stored in h5py file is (7168, 5313)\n",
      "Finished processing batch 0. The number of samples stored in H5PY file so far is 7168\n",
      "Got the encoded sequence\n",
      "Size of enformer output to be stored in h5py file is (7168, 5313)\n",
      "Finished processing batch 1. The number of samples stored in H5PY file so far is 14336\n",
      "Got the encoded sequence\n",
      "Size of enformer output to be stored in h5py file is (7168, 5313)\n",
      "Finished processing batch 2. The number of samples stored in H5PY file so far is 21504\n",
      "Got the encoded sequence\n",
      "Size of enformer output to be stored in h5py file is (7168, 5313)\n",
      "Finished processing batch 3. The number of samples stored in H5PY file so far is 28672\n",
      "Got the encoded sequence\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb Cell 8\u001B[0m line \u001B[0;36m3\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39m__name__\u001B[39m \u001B[39m==\u001B[39m \u001B[39m'\u001B[39m\u001B[39m__main__\u001B[39m\u001B[39m'\u001B[39m:\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001B[0m     \u001B[39mprint\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mStart time is \u001B[39m\u001B[39m{\u001B[39;00mtime\u001B[39m.\u001B[39mtime()\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m)\n\u001B[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001B[0m     storeEnformerOutput()\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001B[0m     \u001B[39mprint\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mEnd time is \u001B[39m\u001B[39m{\u001B[39;00mtime\u001B[39m.\u001B[39mtime()\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m)\n",
      "\u001B[1;32m/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb Cell 8\u001B[0m line \u001B[0;36m4\n\u001B[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001B[0m encodedSequence \u001B[39m=\u001B[39m encodedSequence\u001B[39m.\u001B[39mto(device)\n\u001B[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001B[0m \u001B[39m#Will be of the shape [batch_size * 10626]\u001B[39;00m\n\u001B[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001B[0m enformerPrediction \u001B[39m=\u001B[39m getEnformerPredictions(enformerModel, encodedSequence, ntracks)\u001B[39m.\u001B[39mdetach()\u001B[39m.\u001B[39mcpu()\u001B[39m.\u001B[39mnumpy()\n\u001B[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001B[0m \u001B[39m#The data is getting too big to load, round off enformer predictions to 3 decimal places. \u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001B[0m enformerPrediction \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39maround(enformerPrediction, decimals\u001B[39m=\u001B[39m\u001B[39m3\u001B[39m)\n",
      "\u001B[1;32m/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb Cell 8\u001B[0m line \u001B[0;36m1\n\u001B[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001B[0m     \u001B[39mfor\u001B[39;00m j \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(nbins):\n\u001B[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001B[0m         single_bin_track \u001B[39m=\u001B[39m full_enformer_output[i, j, :]\u001B[39m.\u001B[39mview(\u001B[39m1\u001B[39m, \u001B[39m-\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001B[0m         final_enformer_output \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39;49mcat((final_enformer_output, single_bin_track), dim \u001B[39m=\u001B[39;49m \u001B[39m0\u001B[39;49m)\n\u001B[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc05/hpc/compgen/projects/fragclass/analysis/mvivekanandan/script/madhu_scripts/one_time_use_side_scripts/generate_enformer_tracks_chr1_reference_genome.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001B[0m \u001B[39mreturn\u001B[39;00m final_enformer_output[\u001B[39m1\u001B[39m:]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(f\"Start time is {time.time()}\")\n",
    "    storeEnformerOutput()\n",
    "    print(f\"End time is {time.time()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fragenv] *",
   "language": "python",
   "name": "conda-env-fragenv-py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
